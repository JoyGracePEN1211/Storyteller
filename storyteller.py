# -*- coding: utf-8 -*-
"""Storyteller.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UN79sn375UOnc8c-a-mhVU0XML1CL5k8
"""

!pip install torch torchvision torchaudio
!pip install transformers diffusers
!pip install moviepy gtts pillow imageio[ffmpeg] scikit-image
!pip install opencv-python ffmpeg-python

!pip uninstall numpy -y
!pip install numpy==1.26.0

!pip uninstall tensorflow -y
!pip install tensorflow==2.18.0

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/AliaksandrSiarohin/first-order-model.git
# %cd first-order-model
!pip install -r requirements.txt
!mkdir checkpoints
!wget -O checkpoints/vox-adv-cpk.pth.tar https://www.dropbox.com/s/7j1xl8tx5lw6uyr/vox-adv-cpk.pth.tar?dl=1
# %cd ..

!pip install --upgrade pip setuptools wheel cython numpy scipy joblib threadpoolctl
!pip install scikit-learn

!pip install numpy==1.26.4

!pip install --upgrade scikit-learn tensorflow

!pwd
!ls
!realpath first-order-model

!pip install ffmpeg-python

!rm -rf first-order-model
!git clone https://github.com/AliaksandrSiarohin/first-order-model.git

!cd first-order-model && ls

!pip install --upgrade pip setuptools wheel

!cd first-order-model && pip install -r requirements.txt

!pip install --no-cache-dir --only-binary :all: scikit-learn

!ls first-order-model/

!pip install -r requirements.txt

import ffmpeg

from first-order-model.demo import make_animation

!pip install diffusers
from diffusers import StableDiffusionPipeline
import torch

pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4")
pipe.to("cuda" if torch.cuda.is_available() else "cpu")

import torch
print("GPU Available: ", torch.cuda.is_available())
print("Torch Version: ", torch.__version__)

import os
import re
import torch
import moviepy.editor as mpy
from transformers import pipeline
from diffusers import StableDiffusionPipeline
from gtts import gTTS
from PIL import Image
from first_order_model.demo import make_animation
from skimage import img_as_ubyte
import imageio

def generate_story(prompt, max_length=300):
    print("\nGenerating story...")
    story_gen = pipeline("text-generation", model="gpt2")
    story = story_gen(prompt, max_length=max_length, num_return_sequences=1)[0]["generated_text"]
    return story

def modify_story(story):
    print("\nModifying story...")
    modified_story = re.sub(r"\bwarrior\b", "brave adventurer", story)
    return modified_story

def generate_images_from_story(story, num_frames=3):
    print("\nGenerating images using Stable Diffusion...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4").to(device)

    sentences = story.split(".")[:num_frames]
    images = []

    if not os.path.exists("story_frames"):
        os.makedirs("story_frames")

    for i, sentence in enumerate(sentences):
        if sentence.strip():
            print(f"Generating image for: {sentence}")
            image = pipe(sentence).images[0]
            image_path = f"story_frames/frame_{i}.png"
            image.save(image_path)
            images.append(image_path)

    return images

def text_to_speech(story, output_audio="story_audio.mp3"):
    print("\nConverting text to speech...")
    tts = gTTS(text=story, lang="en")
    tts.save(output_audio)
    return output_audio

def animate_images(source_image, driving_video, output_video="animated_story.mp4"):
    print("\nAnimating images using First Order Motion Model...")
    from first_order_model.fomm_utils import load_checkpoints

    device = "cuda" if torch.cuda.is_available() else "cpu"
    generator, kp_detector = load_checkpoints(config_path="first-order-model/config/vox-adv-256.yaml",
                                              checkpoint_path="first-order-model/checkpoints/vox-adv-cpk.pth.tar",
                                              device=device)

    source_image = imageio.imread(source_image)
    driving_video = imageio.mimread(driving_video)

    predictions = make_animation(source_image, driving_video, generator, kp_detector, device=device)
    imageio.mimsave(output_video, [img_as_ubyte(frame) for frame in predictions])

    return output_video

def create_final_video(animated_video, audio_file, output_file="final_story_video.mp4"):
    print("\nCombining animation with narration...")
    video = mpy.VideoFileClip(animated_video)
    audio = mpy.AudioFileClip(audio_file)

    if video.duration > audio.duration:
        video = video.subclip(0, audio.duration)

    final_video = video.set_audio(audio)
    final_video.write_videofile(output_file, fps=24)

def run_storytelling_pipeline(prompt):
    story = generate_story(prompt)
    modified_story = modify_story(story)
    images = generate_images_from_story(modified_story)
    audio_file = text_to_speech(modified_story)

    if images:
        animated_video = animate_images(images[0], "first-order-model/data/driving.mp4")
        create_final_video(animated_video, audio_file)
        print("Story generated with GAN animation! Output: final_story_video.mp4")
    else:
        print("Failed to generate images!")

if __name__ == "__main__":
    user_prompt = input("Enter a story prompt: ")
    run_storytelling_pipeline(user_prompt)

